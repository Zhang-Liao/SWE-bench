#!/usr/bin/env python3
"""
ä¿å­˜å•ä¸ªSWE-benchæ•°æ®é›†åˆ°JSONæ–‡ä»¶
ä½¿ç”¨æ–¹æ³•: python save_single_dataset.py
"""

from datasets import load_dataset
import json
import os
from pathlib import Path

def save_dataset_to_json(dataset_name="princeton-nlp/SWE-bench", split="test", output_file=None):
    """
    ä¿å­˜æŒ‡å®šæ•°æ®é›†åˆ°JSONæ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        split: æ•°æ®é›†åˆ†å‰² (test/dev)
        output_file: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    """
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name} ({split})")
    
    try:
        # åŠ è½½æ•°æ®é›†
        dataset = load_dataset(dataset_name)
        
        if split not in dataset:
            print(f"âŒ é”™è¯¯: æ•°æ®é›† {dataset_name} æ²¡æœ‰ {split} åˆ†å‰²")
            print(f"å¯ç”¨çš„åˆ†å‰²: {list(dataset.keys())}")
            return
        
        # è·å–æŒ‡å®šåˆ†å‰²çš„æ•°æ®
        data = dataset[split]
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        data_list = [item for item in data]
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_file is None:
            dataset_short = dataset_name.split('/')[-1]
            output_file = f"{dataset_short}_{split}.json"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "swebench_data"
        Path(output_dir).mkdir(exist_ok=True)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        output_path = os.path.join(output_dir, output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / (1024*1024):.2f} MB")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯:")
        print(f"   å¤§å°: {len(data_list)} æ¡è®°å½•")
        print(f"   ç‰¹å¾: {data.features}")
        print(f"   åˆ—å: {data.column_names}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå®ä¾‹çš„é”®
        if data_list:
            print(f"\nğŸ” ç¬¬ä¸€ä¸ªå®ä¾‹çš„å­—æ®µ:")
            for key in data_list[0].keys():
                value = data_list[0][key]
                if isinstance(value, str) and len(value) > 100:
                    print(f"   {key}: {value[:100]}...")
                else:
                    print(f"   {key}: {value}")
        
        return output_path
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•° - ä¿å­˜ä¸»è¦çš„SWE-benchæ•°æ®é›†"""
    print("ğŸš€ SWE-benchæ•°æ®é›†ä¿å­˜å·¥å…·")
    print("=" * 50)
    
    # ä¿å­˜ä¸»è¦çš„SWE-benchæ•°æ®é›†
    datasets_to_save = [
        ("princeton-nlp/SWE-bench", "test"),
        ("princeton-nlp/SWE-bench", "dev"),
        ("SWE-bench/SWE-bench_Lite", "test"),
        ("SWE-bench/SWE-bench_Lite", "dev")
    ]
    
    saved_files = []
    
    for dataset_name, split in datasets_to_save:
        print(f"\n{'='*50}")
        output_file = save_dataset_to_json(dataset_name, split)
        if output_file:
            saved_files.append(output_file)
    
    print(f"\nğŸ‰ å®Œæˆï¼")
    print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
    for file in saved_files:
        print(f"   - {file}")
    
    print(f"\nğŸ’¡ æç¤º: æ‰€æœ‰æ–‡ä»¶éƒ½ä¿å­˜åœ¨ 'swebench_data' ç›®å½•ä¸­")

if __name__ == "__main__":
    main()

