#!/usr/bin/env python3
"""
SWE-benchæ•°æ®é›†ä¿å­˜è„šæœ¬
å°†SWE-benchçš„æ‰€æœ‰æ•°æ®é›†ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­
"""

import json
import os
from datasets import load_dataset
from pathlib import Path
from typing import Dict, Any, List

def save_dataset_to_json(dataset, filename: str, output_dir: str = "swebench_data") -> None:
    """
    å°†æ•°æ®é›†ä¿å­˜ä¸ºJSONæ–‡ä»¶
    
    Args:
        dataset: Hugging Faceæ•°æ®é›†å¯¹è±¡
        filename: è¾“å‡ºæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(exist_ok=True)
    
    # å°†æ•°æ®é›†è½¬æ¢ä¸ºåˆ—è¡¨
    data_list = [item for item in dataset]
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    output_path = os.path.join(output_dir, filename)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å·²ä¿å­˜ {len(data_list)} æ¡è®°å½•åˆ° {output_path}")

def save_dataset_info(dataset, filename: str, output_dir: str = "swebench_data") -> None:
    """
    ä¿å­˜æ•°æ®é›†ä¿¡æ¯åˆ°JSONæ–‡ä»¶
    
    Args:
        dataset: Hugging Faceæ•°æ®é›†å¯¹è±¡
        filename: è¾“å‡ºæ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(exist_ok=True)
    
    # æ”¶é›†æ•°æ®é›†ä¿¡æ¯
    info = {
        "size": len(dataset),
        "features": str(dataset.features),
        "column_names": dataset.column_names,
        "split": getattr(dataset, 'split', 'unknown'),
        "description": getattr(dataset, 'description', ''),
    }
    
    # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
    output_path = os.path.join(output_dir, filename)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å·²ä¿å­˜æ•°æ®é›†ä¿¡æ¯åˆ° {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¸‹è½½å’Œä¿å­˜SWE-benchæ•°æ®é›†...")
    
    # å®šä¹‰è¦ä¸‹è½½çš„æ•°æ®é›†
    datasets_to_download = {
        "SWE-bench": {
            "name": "princeton-nlp/SWE-bench",
            "splits": ["test", "dev"]
        },
        "SWE-bench_Lite": {
            "name": "SWE-bench/SWE-bench_Lite", 
            "splits": ["test", "dev"]
        },
        "SWE-bench_Verified": {
            "name": "SWE-bench/SWE-bench_Verified",
            "splits": ["test", "dev"]
        },
        "SWE-bench_Multimodal": {
            "name": "SWE-bench/SWE-bench_Multimodal",
            "splits": ["test", "dev"]
        }
    }
    
    # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
    main_output_dir = "swebench_data"
    Path(main_output_dir).mkdir(exist_ok=True)
    
    # ä¸‹è½½å’Œä¿å­˜æ¯ä¸ªæ•°æ®é›†
    for dataset_name, config in datasets_to_download.items():
        print(f"\nğŸ“¦ æ­£åœ¨å¤„ç† {dataset_name}...")
        
        try:
            # åŠ è½½æ•°æ®é›†
            dataset = load_dataset(config["name"])
            
            # ä¸ºæ¯ä¸ªsplitä¿å­˜æ•°æ®
            for split in config["splits"]:
                if split in dataset:
                    print(f"  ğŸ“ ä¿å­˜ {split} åˆ†å‰²...")
                    
                    # ä¿å­˜æ•°æ®
                    data_filename = f"{dataset_name}_{split}.json"
                    save_dataset_to_json(dataset[split], data_filename, main_output_dir)
                    
                    # ä¿å­˜ä¿¡æ¯
                    info_filename = f"{dataset_name}_{split}_info.json"
                    save_dataset_info(dataset[split], info_filename, main_output_dir)
                else:
                    print(f"  âš ï¸  {split} åˆ†å‰²ä¸å­˜åœ¨äº {dataset_name}")
                    
        except Exception as e:
            print(f"  âŒ ä¸‹è½½ {dataset_name} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # åˆ›å»ºæ€»ä½“ç»Ÿè®¡æ–‡ä»¶
    print(f"\nğŸ“Š åˆ›å»ºæ€»ä½“ç»Ÿè®¡æ–‡ä»¶...")
    create_summary_stats(main_output_dir)
    
    print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†å·²ä¿å­˜åˆ° {main_output_dir} ç›®å½•ï¼")
    print(f"ğŸ“ ç›®å½•ç»“æ„:")
    print(f"   {main_output_dir}/")
    print(f"   â”œâ”€â”€ SWE-bench_test.json")
    print(f"   â”œâ”€â”€ SWE-bench_test_info.json")
    print(f"   â”œâ”€â”€ SWE-bench_dev.json")
    print(f"   â”œâ”€â”€ SWE-bench_dev_info.json")
    print(f"   â”œâ”€â”€ SWE-bench_Lite_test.json")
    print(f"   â”œâ”€â”€ SWE-bench_Lite_test_info.json")
    print(f"   â”œâ”€â”€ ...")
    print(f"   â””â”€â”€ summary_stats.json")

def create_summary_stats(output_dir: str) -> None:
    """åˆ›å»ºæ€»ä½“ç»Ÿè®¡æ–‡ä»¶"""
    stats = {
        "total_datasets": 0,
        "total_instances": 0,
        "datasets": {}
    }
    
    # æ‰«æè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    for file_path in Path(output_dir).glob("*.json"):
        if file_path.name.endswith("_info.json"):
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            dataset_name = file_path.stem
            stats["datasets"][dataset_name] = {
                "file_path": str(file_path),
                "instance_count": len(data),
                "file_size_mb": round(file_path.stat().st_size / (1024 * 1024), 2)
            }
            stats["total_instances"] += len(data)
            stats["total_datasets"] += 1
            
        except Exception as e:
            print(f"  âš ï¸ å¤„ç† {file_path} æ—¶å‡ºé”™: {str(e)}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = os.path.join(output_dir, "summary_stats.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å·²ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° {stats_path}")
    print(f"ğŸ“ˆ æ€»è®¡: {stats['total_datasets']} ä¸ªæ•°æ®é›†, {stats['total_instances']} ä¸ªå®ä¾‹")

if __name__ == "__main__":
    main()

