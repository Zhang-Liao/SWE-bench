#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆSWE-benchæ•°æ®é›†ä¿å­˜è„šæœ¬
åªä¿å­˜ä¸»è¦çš„SWE-benchæ•°æ®é›†åˆ°JSONæ–‡ä»¶
"""

import json
import os
from datasets import load_dataset
from pathlib import Path

def save_swebench_data():
    """ä¿å­˜SWE-benchä¸»è¦æ•°æ®é›†"""
    print("ğŸš€ å¼€å§‹ä¸‹è½½å’Œä¿å­˜SWE-benchæ•°æ®é›†...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "swebench_data"
    Path(output_dir).mkdir(exist_ok=True)
    
    # è¦ä¸‹è½½çš„æ•°æ®é›†åˆ—è¡¨
    datasets = [
        ("princeton-nlp/SWE-bench", "SWE-bench"),
        ("SWE-bench/SWE-bench_Lite", "SWE-bench_Lite"),
        ("SWE-bench/SWE-bench_Verified", "SWE-bench_Verified")
    ]
    
    total_instances = 0
    
    for dataset_name, file_prefix in datasets:
        print(f"\nğŸ“¦ æ­£åœ¨å¤„ç† {file_prefix}...")
        
        try:
            # åŠ è½½æ•°æ®é›†
            dataset = load_dataset(dataset_name)
            
            # ä¿å­˜æ¯ä¸ªåˆ†å‰²
            for split_name in ["test", "dev"]:
                if split_name in dataset:
                    print(f"  ğŸ“ ä¿å­˜ {split_name} åˆ†å‰²...")
                    
                    # è½¬æ¢ä¸ºåˆ—è¡¨
                    data_list = [item for item in dataset[split_name]]
                    
                    # ä¿å­˜ä¸ºJSON
                    filename = f"{file_prefix}_{split_name}.json"
                    filepath = os.path.join(output_dir, filename)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(data_list, f, indent=2, ensure_ascii=False)
                    
                    print(f"    âœ… å·²ä¿å­˜ {len(data_list)} æ¡è®°å½•åˆ° {filename}")
                    total_instances += len(data_list)
                    
                    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
                    info = {
                        "dataset_name": dataset_name,
                        "split": split_name,
                        "size": len(data_list),
                        "features": str(dataset[split_name].features),
                        "column_names": dataset[split_name].column_names
                    }
                    
                    info_filename = f"{file_prefix}_{split_name}_info.json"
                    info_filepath = os.path.join(output_dir, info_filename)
                    
                    with open(info_filepath, 'w', encoding='utf-8') as f:
                        json.dump(info, f, indent=2, ensure_ascii=False)
                    
                    print(f"    âœ… å·²ä¿å­˜æ•°æ®é›†ä¿¡æ¯åˆ° {info_filename}")
                else:
                    print(f"  âš ï¸  {split_name} åˆ†å‰²ä¸å­˜åœ¨")
                    
        except Exception as e:
            print(f"  âŒ ä¸‹è½½ {file_prefix} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # åˆ›å»ºæ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“Š åˆ›å»ºæ€»ä½“ç»Ÿè®¡...")
    stats = {
        "total_instances": total_instances,
        "output_directory": output_dir,
        "files_created": len(list(Path(output_dir).glob("*.json")))
    }
    
    stats_path = os.path.join(output_dir, "summary.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ å®Œæˆï¼")
    print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ“ˆ æ€»è®¡: {total_instances} ä¸ªå®ä¾‹")
    print(f"ğŸ“„ æ–‡ä»¶æ•°: {stats['files_created']} ä¸ªJSONæ–‡ä»¶")

if __name__ == "__main__":
    save_swebench_data()

